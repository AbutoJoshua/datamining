{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joshua Abuto\n",
    "#1001530342\n",
    "\n",
    "import math\n",
    "import os, nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# variables\n",
    "docdict = dict()  # dictionary of {doc : {word1 : word1_num, word2 : word2_num, .... }}\n",
    "dicttf = dict()  # Dictionary of {word : [doc1, doc3, ... ]}\n",
    "wtfidfdict = dict()  # dictionary for weighted tf-idf vector\n",
    "wqdict = dict()  # dictionary for weight query\n",
    "tfdictionary = dict()  # tf dictionary\n",
    "postinglist = dict()  # posting list\n",
    "num_of_docs = 0\n",
    "simcos = dict()  # similarity cosine dictionary\n",
    "\n",
    "\n",
    "# I was meant to break down all the methods but I did not want to ruin my code\n",
    "\n",
    "def main():\n",
    "    corpusroot = './presidential_debates/presidential_debates'\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'[A-Z|a-z|0-9]*')\n",
    "\n",
    "    for i in range(0, 30):\n",
    "        dicttf[i] = dict()\n",
    "        wtfidfdict[i] = dict()\n",
    "        simcos[i] = dict()\n",
    "\n",
    "    j = 0\n",
    "    # reading the code and stemming each token\n",
    "    for filename in os.listdir(corpusroot):\n",
    "        file = open(os.path.join(corpusroot, filename), \"r\", encoding='UTF-8')\n",
    "        doc = file.read()\n",
    "        file.close()\n",
    "        doc = doc.lower()\n",
    "\n",
    "        # Tokenizing the document\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        token_list = [s.strip(' ') for s in tokens]\n",
    "        # print(tokens)\n",
    "        # print(sorted(tokens))\n",
    "\n",
    "        # Removing the stop words and the spaces\n",
    "        filtered_sentence = [w for w in token_list if not w in stop_words]\n",
    "        filtered_sentence = [w for w in filtered_sentence if not w in '']\n",
    "\n",
    "        # Stemming the filtered words\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_words = []\n",
    "        for token in filtered_sentence:\n",
    "            stemmed_words.append(stemmer.stem(token))\n",
    "        # print(stemmed_words)\n",
    "\n",
    "        # Calculating the TF for each term in each document {doc : {word1 : word1_num, word2 : word2_num, .... }}\n",
    "        for word in stemmed_words:\n",
    "            if word in dicttf:\n",
    "                dicttf[j][word] += 1\n",
    "            else:\n",
    "                dicttf[j][word] = 1\n",
    "\n",
    "        # Calculating the tf for each query for all documents\n",
    "        for word in stemmed_words:\n",
    "            if word in tfdictionary:\n",
    "                tfdictionary[word] += 1\n",
    "            else:\n",
    "                tfdictionary[word] = 1\n",
    "\n",
    "            # {word : [doc1, doc3, ... ]}\n",
    "            text_str = str(i) + '.txt'\n",
    "            if word in docdict:\n",
    "                if text_str not in docdict[word]:\n",
    "                    docdict[word].append(text_str)\n",
    "\n",
    "            else:\n",
    "                docdict[word] = list()\n",
    "                docdict[word].append(text_str)\n",
    "\n",
    "            j += 1\n",
    "    # going through all documents for calculations\n",
    "    for j in range(0, 30):\n",
    "        for word in dicttf:\n",
    "            if word not in dicttf[j][word]:\n",
    "                wtfidfdict[j][word] = (1 + math.log(dicttf[j][word], 10)) * (math.log(30 / tfdictionary[word]))\n",
    "\n",
    "        for word in tfdictionary:\n",
    "            if word not in wqdict:\n",
    "                wqdict[word] = (1 + math.log(tfdictionary[word]), 10)\n",
    "\n",
    "        for k in range(0, len(wtfidfdict[j]) - 1):\n",
    "            sorted(wtfidfdict[j][word], key=word, reverse=True);\n",
    "\n",
    "        for l in range(0, 9):\n",
    "            if word not in postinglist:\n",
    "                postinglist[word] = word\n",
    "                postinglist[word][l] = j\n",
    "\n",
    "        for word in wqdict:\n",
    "            if word in postinglist:\n",
    "                if j in postinglist[word]:\n",
    "                    simcos[j][word] = wqdict[word] * wtfidfdict[j][word]\n",
    "                else:\n",
    "                    simcos[j][word] += wqdict[word] * wtfidfdict[j][postinglist[word][9]]\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        print(simcos);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
